# Default config for attentive probe evaluation
# This config is used when running main_probe.py standalone

# Task configuration
task: classification  # classification or regression
num_classes: 2

# Model configuration
model_name: vit_base
attn_mode: flash_attn
add_w: mapping
crop_size: [450, 160]
patch_size: 16
pred_depth: 12
pred_emb_dim: 384
gradient_checkpointing: false
global_pool: false

# Data configuration
data_make_fn: hca_sex  # hca_sex or hcya_sex
data_path: 's3://medarc/fmri-fm/datasets/hcp-parc-v2'  # Path to processed dataset directory (or s3:// path)
use_normalization: true
downsample: true
make_constant: false

# Dataset names (for logging/evaluation)
train_dataset: train
val_dataset: valid
test_dataset: test
eval_datasets: []

# Probe-specific configuration
representations: [avg_patch, patch]  # Representations to probe
lr_scale_grid: [0.1, 0.3, 1.0, 3.0, 10.0, 30.0, 100.0]
weight_decay_grid: [3e-4, 0.001, 0.01, 0.03, 0.1, 0.3, 1.0]
attn_pool_embed_dim: null  # Embedding dimension for attention pooling (optional)

# Training configuration
batch_size: 32
accum_iter: 4
epochs: 20
warmup_epochs: 2
start_epoch: 0
lr: null  # If null, will be computed from base_lr
# absolute_lr = base_lr * total_batch_size / 256
base_lr: 1e-3
# lower lr bound for cyclic schedulers that hit 0
min_lr: 0.0
weight_decay: 0.05
betas: [0.9, 0.95]

# Optimization
clip_grad: 1.0
amp: false
amp_dtype: float16

# Checkpointing
load_path: '/teamspace/gcs_folders/share/fmri-fm/brain-jepa/jepa-ep300.pth.tar'  # Path to pretrained checkpoint
pretrain_ckpt: ''  # Alternative name for load_path
finetune: false  # If true, finetune backbone; if false, freeze it

# Output configuration
output_dir: ./output_dir/attentive_probe
name: AP  # Experiment name (optional)
log_dir: null  # Log directory (optional)

# Distributed training
distributed: false
dist_on_itp: false
gpu: 0
device: cuda
world_size: 1
rank: 0
local_rank: -1
dist_url: null
dist_backend: nccl

# Misc
seed: 0
num_workers: 8
pin_mem: true
debug: false
print_freq: 20
presend_cuda: false

# Wandb (optional)
wandb: true
wandb_entity: shamussim
wandb_project: brain-jepa-AP

